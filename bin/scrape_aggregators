#!/usr/bin/env python
"""Simple commandline interface for scraping aggregator sites for live stream URLs."""
import argparse
import logging
import time
from streamscrape import other
from streamscrape import utils

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="""
        Script to scrape several major aggregators for live stream URLS.
        """
    )
    parser.add_argument(
        "-p",
        "--preview",
        dest="preview",
        action="store_true",
        help="Print URLs to stdout rather than saving to DB.",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        dest="verbose",
        action="store_true",
        help="Output INFO level logging.",
    )
    parser.add_argument(
        "-vv",
        "--veryverbose",
        dest="debug",
        action="store_true",
        help="Output DEBUG level logging.",
    )
    args = parser.parse_args()

    if args.debug:
        log_level = logging.DEBUG
    elif args.verbose:
        log_level = logging.INFO
    else:
        log_level = logging.ERROR

    # Configure logging for this application
    log = logging.getLogger("streamscrape")
    log.propagate = 0  # prevent propagation to the root logger
    ch = logging.StreamHandler()
    log.setLevel(log_level)
    ch.setLevel(log_level)
    formatter = logging.Formatter("[%(levelname)s] %(name)s - %(message)s")
    ch.setFormatter(formatter)
    log.addHandler(ch)

    logger = logging.getLogger(__name__)
    if args.preview:
        logger.warning("Running in preview mode. Records will not save to DB.")

    # Call the main routine
    result = other.scrape()
    for aggregator in result:
        for agg, urls in aggregator.items():
            if args.preview:
                for url in urls:
                    print(
                        "[{}][{}] {} ({})".format(
                            time.strftime(
                                "%Y-%m-%d %H:%M:%S", time.gmtime(url["timestamp"])
                            ),
                            agg,
                            url["url"],
                            url["ip"],
                        )
                    )
            else:
                utils.save_urls_to_db(urls, agg)
                utils.update_last_scanned(agg, int(time.time()))
